{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272f15ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T17:01:37.305116Z",
     "iopub.status.busy": "2021-05-25T17:01:37.304898Z",
     "iopub.status.idle": "2021-05-25T17:01:37.307735Z",
     "shell.execute_reply": "2021-05-25T17:01:37.307032Z",
     "shell.execute_reply.started": "2021-05-25T17:01:37.305094Z"
    }
   },
   "outputs": [],
   "source": [
    "import sparkhub.client as sh\n",
    "\n",
    "spark = sh.cluster(name=\"thirst-quenching-tail\", max_workers=50)\n",
    "\n",
    "sh.ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde84204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-06T00:19:30.568091Z",
     "iopub.status.busy": "2021-05-06T00:19:30.567836Z",
     "iopub.status.idle": "2021-05-06T00:19:30.571181Z",
     "shell.execute_reply": "2021-05-06T00:19:30.570693Z",
     "shell.execute_reply.started": "2021-05-06T00:19:30.568068Z"
    }
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F, types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705a4ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# code snippets used in the slides\n",
    "\n",
    "# # monotonically increasing ids\n",
    "# mono_ids = data\\\n",
    "#     .select('col').distinct()\\\n",
    "#     .withColumn('col_id', F.monotonically_increasing_id())\n",
    "# data\\\n",
    "#     .join(F.broadcast(mono_ids), on='col')\\\n",
    "#     .drop('col')\n",
    "\n",
    "# # unique values counts\n",
    "# data\\\n",
    "#     .groupby('id', 'value').count()\\\n",
    "#     .withColumnRenamed('count', 'value_count')\\\n",
    "#     .withColumn('value_count_pair', F.struct('value', 'value_count'))\\\n",
    "#     .groupby('id')\\\n",
    "#     .agg(F.collect_list('value_count_pair').alias('value_count_pairs'))\n",
    "\n",
    "# # time series data\n",
    "# data\\\n",
    "#     .withColumn('time_transaction_pair', F.struct('time', 'transaction'))\\\n",
    "#     .groupby('id')\\\n",
    "#     .agg(F.collect_list('time_transaction_pair').alias('time_transaction_pairs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d2f20",
   "metadata": {},
   "source": [
    "At quantcast, one of our most expensive queries look into pairwise interactions between pairs of events. A lot of the queries roughly look like\n",
    "* Aggregating all of the data\n",
    "* Determining some `partition_key` that splits the data into sufficiently small parititions such that looking at pairwise interactions is computationally feasible.\n",
    "* Look at all of the pairwise interactions between events for each `partition_key`\n",
    "* Extract some useful information from each of these pair of events. Sometimes with machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa54e8",
   "metadata": {},
   "source": [
    "### Overview of our query evolution\n",
    "In this section we show:\n",
    "* An example of processing these pairwise interactions at scale on sanitized data\n",
    "* Examples of how we applied the following optimizations:\n",
    "    * Using efficient python packages to improve over our pandas implementation by 10x\n",
    "    * Redesigning our query to use scalar UDFs instead of grouped map UDFs to improve our implementation by 12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1880611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-06T00:20:16.192333Z",
     "iopub.status.busy": "2021-05-06T00:20:16.192097Z",
     "iopub.status.idle": "2021-05-06T00:20:20.921637Z",
     "shell.execute_reply": "2021-05-06T00:20:20.921058Z",
     "shell.execute_reply.started": "2021-05-06T00:20:16.192312Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    # parameters for sanitized data\n",
    "    n = 1000000\n",
    "    partition_size = 20\n",
    "    d = 20\n",
    "\n",
    "    # function that generates random numbers in the range of [-1, 1)\n",
    "    def generate_random_vector(size):\n",
    "        return (2 * np.random.random(size)) - 1\n",
    "\n",
    "    # generate sample data\n",
    "    sample_data_1M = pd.DataFrame([[i // partition_size, i] for i in range(n)], columns=['partition_key', 'event_id'])\n",
    "    sample_data_1M['feature_vector'] = generate_random_vector((n, d)).tolist()\n",
    "\n",
    "    sample_data_1M.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3655a224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-06T00:20:26.221200Z",
     "iopub.status.busy": "2021-05-06T00:20:26.220955Z",
     "iopub.status.idle": "2021-05-06T00:20:26.231862Z",
     "shell.execute_reply": "2021-05-06T00:20:26.231422Z",
     "shell.execute_reply.started": "2021-05-06T00:20:26.221178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition_key</th>\n",
       "      <th>event_id</th>\n",
       "      <th>feature_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.5165411328143239, -0.7697248833258343, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.3785918054897246, -0.8154872951239256, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.6884332215238746, -0.532683039200122, 0.30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8309149840909955, -0.896161813570687, 0.511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.7258664613993673, -0.1955426784967571, -0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   partition_key  event_id                                     feature_vector\n",
       "0              0         0  [-0.5165411328143239, -0.7697248833258343, 0.2...\n",
       "1              0         1  [-0.3785918054897246, -0.8154872951239256, 0.6...\n",
       "2              0         2  [-0.6884332215238746, -0.532683039200122, 0.30...\n",
       "3              0         3  [0.8309149840909955, -0.896161813570687, 0.511...\n",
       "4              0         4  [0.7258664613993673, -0.1955426784967571, -0.8..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sample_data_1M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4e79bcf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:28:26.711479Z",
     "iopub.status.busy": "2021-05-04T00:28:26.711245Z",
     "iopub.status.idle": "2021-05-04T00:28:26.716172Z",
     "shell.execute_reply": "2021-05-04T00:28:26.715349Z",
     "shell.execute_reply.started": "2021-05-04T00:28:26.711455Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample machine learning model implementation that operates on these pairs of events\n",
    "hidden_layer_size = 10\n",
    "first_layer = generate_random_vector((d * 2, hidden_layer_size))\n",
    "final_layer = generate_random_vector(hidden_layer_size)\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def score_feature_pair(feature_vector1, feature_vector2):\n",
    "    hidden_layer = np.dot(np.concatenate([feature_vector1, feature_vector2]), first_layer)\n",
    "    non_linear_result = np.maximum(hidden_layer, 0)\n",
    "    return sigmoid(np.dot(non_linear_result, final_layer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82b83c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:28:31.170829Z",
     "iopub.status.busy": "2021-05-04T00:28:31.170595Z",
     "iopub.status.idle": "2021-05-04T00:28:31.329556Z",
     "shell.execute_reply": "2021-05-04T00:28:31.328927Z",
     "shell.execute_reply.started": "2021-05-04T00:28:31.170806Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a spark dataframe for this data\n",
    "sample_data_1M_df = spark.createDataFrame(sample_data_1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "34f99aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:52:03.006749Z",
     "iopub.status.busy": "2021-05-04T00:52:03.006511Z",
     "iopub.status.idle": "2021-05-04T00:52:03.039283Z",
     "shell.execute_reply": "2021-05-04T00:52:03.038691Z",
     "shell.execute_reply.started": "2021-05-04T00:52:03.006724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partition_key: int, event_id1: int, event_id2: int, model_score: double]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v1 of our code. Strategy was to use pandas grouped map UDFs\n",
    "# on our small sample this takes 20 min CPU time\n",
    "\n",
    "    # schema of our UDF\n",
    "    process_partition_schema = T.StructType([\n",
    "        T.StructField('partition_key', T.IntegerType()),\n",
    "        T.StructField('event_id1', T.IntegerType()),\n",
    "        T.StructField('event_id2', T.IntegerType()),\n",
    "        T.StructField('model_score', T.DoubleType()),\n",
    "    ])\n",
    "\n",
    "    # processes a single partition_key of data\n",
    "    def process_partition(data):\n",
    "        output = []\n",
    "\n",
    "        # uses pandas iterrows to do the double for loop and score everything\n",
    "        partition_key = data.iloc[0]['partition_key']\n",
    "        for i, row1 in data.iterrows():\n",
    "            event_id1 = row1['event_id']\n",
    "            for j, row2 in data[i+1:].iterrows():\n",
    "                event_id2 = row2['event_id']\n",
    "                model_score = score_feature_pair(row1['feature_vector'], row2['feature_vector'])\n",
    "                output.append([partition_key, event_id1, event_id2, model_score])\n",
    "\n",
    "        return pd.DataFrame(output, columns=process_partition_schema.names)\n",
    "\n",
    "    sample_data_1M_df.groupby('partition_key').applyInPandas(process_partition, process_partition_schema)\\\n",
    "        .coalesce(1)\\\n",
    "        .write.mode('overwrite').parquet('/qfs/tmp/mtong/spark_demo_base')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "406385d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:52:05.397280Z",
     "iopub.status.busy": "2021-05-04T00:52:05.397048Z",
     "iopub.status.idle": "2021-05-04T00:54:18.176524Z",
     "shell.execute_reply": "2021-05-04T00:54:18.175941Z",
     "shell.execute_reply.started": "2021-05-04T00:52:05.397257Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Sample scoring function\n",
    "# Note how we are using numpy types instead of pandas types.\n",
    "# This already provides us a 6x speedup over this dataset\n",
    "process_partition_schema = T.StructType([\n",
    "    T.StructField('partition_key', T.IntegerType()),\n",
    "    T.StructField('event_id1', T.IntegerType()),\n",
    "    T.StructField('event_id2', T.IntegerType()),\n",
    "    T.StructField('model_score', T.DoubleType()),\n",
    "])\n",
    "\n",
    "def process_partition(data):\n",
    "    output = []\n",
    "\n",
    "    # easy way to convert pandas series to numpy vectors and matrices\n",
    "    event_ids = data['event_id'].values\n",
    "    # convert to numpy matrix so each individual element is a numpy vector\n",
    "    feature_matrix = np.array(data['feature_vector'].values.tolist())\n",
    "\n",
    "    # use of enumerate and zip instead of using pandas iterrows\n",
    "    partition_key = data.iloc[0]['partition_key']\n",
    "    for i, (event_id1, feature_vector1) in enumerate(zip(event_ids, feature_matrix)):\n",
    "        for event_id2, feature_vector2 in zip(event_ids[i+1:], feature_matrix[i+1:]):\n",
    "            model_score = score_feature_pair(feature_vector1, feature_vector2)\n",
    "            output.append([partition_key, event_id1, event_id2, model_score])\n",
    "\n",
    "    return pd.DataFrame(output, columns=process_partition_schema.names)\n",
    "\n",
    "sample_data_1M_df.groupby('partition_key').applyInPandas(process_partition, process_partition_schema)\\\n",
    "    .coalesce(1)\\\n",
    "    .write.mode('overwrite').parquet('/qfs/tmp/mtong/spark_demo_numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ddb63d8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T00:54:18.236810Z",
     "iopub.status.busy": "2021-05-04T00:54:18.236652Z",
     "iopub.status.idle": "2021-05-04T00:54:26.700884Z",
     "shell.execute_reply": "2021-05-04T00:54:26.700317Z",
     "shell.execute_reply.started": "2021-05-04T00:54:18.236790Z"
    }
   },
   "outputs": [],
   "source": [
    "# v3 of our code.\n",
    "# Main insight is that we can process multiple partition_keys at once with pandas udfs\n",
    "# This runs in about 10x faster than the previous version of our solution\n",
    "\n",
    "\n",
    "    process_partition_schema = T.StructType([\n",
    "        T.StructField('partition_key', T.ArrayType(T.IntegerType())),\n",
    "        T.StructField('event_id1', T.ArrayType(T.IntegerType())),\n",
    "        T.StructField('event_id2', T.ArrayType(T.IntegerType())),\n",
    "        T.StructField('model_score', T.ArrayType(T.DoubleType())),\n",
    "    ])\n",
    "\n",
    "    # our first iteration of attempting pandas UDFs was to write a function for processing each row\n",
    "    def process_partition(partition_key, event_ids, feature_matrix):\n",
    "        feature_matrix = np.array(feature_matrix.tolist())\n",
    "        output = []\n",
    "        for i, (event_id1, feature_vector1) in enumerate(zip(event_ids, feature_matrix)):\n",
    "            for event_id2, feature_vector2 in zip(event_ids[i+1:], feature_matrix[i+1:]):\n",
    "                model_score = score_feature_pair(feature_vector1, feature_vector2)\n",
    "                output.append([partition_key, event_id1, event_id2, model_score])\n",
    "        # massage columns so they are arrays of fields\n",
    "        return list(zip(*output))\n",
    "\n",
    "    # and a separate function that could process all rows\n",
    "    def process_partition_batch(partition_key_series, event_ids_series, feature_matrix_series):\n",
    "        results = [process_partition(*args) for args in zip(partition_key_series, event_ids_series, feature_matrix_series)]\n",
    "        return pd.DataFrame(results, columns=process_partition_schema.names)\n",
    "\n",
    "    process_partition_udf = F.pandas_udf(process_partition_batch, process_partition_schema)\n",
    "\n",
    "# with pandas UDFs that means we now have to write our queries such\n",
    "# that each partition_key occupies a single row\n",
    "# we also need to massage the data format a bit\n",
    "\n",
    "    sample_data_1M_df\\\n",
    "        .withColumn('event_id_feature_pair', F.struct('event_id', 'feature_vector'))\\\n",
    "        .groupby('partition_key')\\\n",
    "        .agg(F.collect_list('event_id_feature_pair').alias('event_id_feature_pairs'))\\\n",
    "        .withColumn('partition_scores', process_partition_udf(\n",
    "            'partition_key', 'event_id_feature_pairs.event_id', 'event_id_feature_pairs.feature_vector'))\\\n",
    "        .repartition(1)\\\n",
    "        .withColumn('partition_scores_zipped', F.arrays_zip(*[f'partition_scores.{col}' for col in process_partition_schema.names]))\\\n",
    "        .withColumn('partition_scores_exploded', F.explode('partition_scores_zipped'))\\\n",
    "        .select(*[F.col(f'partition_scores_exploded.{i}').alias(col) for i, col in enumerate(process_partition_schema.names)])\\\n",
    "        .write.mode('overwrite').parquet('/qfs/tmp/mtong/spark_demo_scalar_udf')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6c195",
   "metadata": {},
   "source": [
    "### Overview of individual function improvements\n",
    "In this section we show:\n",
    "* How scoring things in batches improves our overall scoring algorithm\n",
    "* An example of how we use jit to improve our model scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b8f52b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T01:26:21.770464Z",
     "iopub.status.busy": "2021-05-04T01:26:21.770224Z",
     "iopub.status.idle": "2021-05-04T01:26:21.801868Z",
     "shell.execute_reply": "2021-05-04T01:26:21.801255Z",
     "shell.execute_reply.started": "2021-05-04T01:26:21.770440Z"
    }
   },
   "outputs": [],
   "source": [
    "# intialize some data\n",
    "# typically in each pandas_udf batch we score ~100k rows at a time\n",
    "feature_matrix1 = np.random.random((100000, d))\n",
    "feature_matrix2 = np.random.random((100000, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8bc55a79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T01:31:09.744243Z",
     "iopub.status.busy": "2021-05-04T01:31:09.744005Z",
     "iopub.status.idle": "2021-05-04T01:31:16.477408Z",
     "shell.execute_reply": "2021-05-04T01:31:16.476776Z",
     "shell.execute_reply.started": "2021-05-04T01:31:09.744220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837 ms ± 23.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# baseline method, score each row individually\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def score_feature_pair(feature_vector1, feature_vector2):\n",
    "    hidden_layer = np.dot(np.concatenate([feature_vector1, feature_vector2]), first_layer)\n",
    "    non_linear_result = np.maximum(hidden_layer, 0)\n",
    "    return sigmoid(np.dot(non_linear_result, final_layer))\n",
    "\n",
    "%timeit _ = [score_feature_pair(*args) for args in zip(feature_matrix1, feature_matrix2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "311b1c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T01:31:20.487862Z",
     "iopub.status.busy": "2021-05-04T01:31:20.487452Z",
     "iopub.status.idle": "2021-05-04T01:31:28.633034Z",
     "shell.execute_reply": "2021-05-04T01:31:28.632156Z",
     "shell.execute_reply.started": "2021-05-04T01:31:20.487808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 ms ± 11.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# by writing scoring as a batch operation, we can take advantage of numpy optimizations\n",
    "# this makes the function \n",
    "def score_feature_matrix(feature_matrix1, feature_matrix2):\n",
    "    hidden_layer = np.dot(np.hstack([feature_matrix1, feature_matrix2]), first_layer)\n",
    "    non_linear_result = np.maximum(hidden_layer, 0)\n",
    "    return sigmoid(np.dot(non_linear_result, final_layer))\n",
    "\n",
    "%timeit _ = score_feature_matrix(feature_matrix1, feature_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f29af804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T01:38:17.794586Z",
     "iopub.status.busy": "2021-05-04T01:38:17.794253Z",
     "iopub.status.idle": "2021-05-04T01:38:24.136041Z",
     "shell.execute_reply": "2021-05-04T01:38:24.135394Z",
     "shell.execute_reply.started": "2021-05-04T01:38:17.794547Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 ms ± 5.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# one of the downsides to using jit is that not all numpy functions are supported\n",
    "# so sometimes you have to do workarounds\n",
    "# however there is almost always a way to rewrite numpy functions to faster jit versions\n",
    "@jit(nopython=True)\n",
    "def sigmoid_jit(vector):\n",
    "    return 1 / (1 + np.exp(-vector))\n",
    "\n",
    "@jit(nopython=True)\n",
    "def score_feature_matrix_jit(stacked_matrix):\n",
    "    hidden_layer = np.dot(stacked_matrix, first_layer)\n",
    "    non_linear_result = np.maximum(hidden_layer, 0)\n",
    "    return sigmoid_jit(np.dot(non_linear_result, final_layer))\n",
    "\n",
    "# compile the jit function\n",
    "_ = score_feature_matrix_jit(np.hstack([feature_matrix1, feature_matrix2]))\n",
    "%timeit _ = score_feature_matrix_jit(np.hstack([feature_matrix1, feature_matrix2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "notebook_metadata_filter": "all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
